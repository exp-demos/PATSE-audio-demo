<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>
    Position-Aware Target Speaker Extraction for Long-Form Multi-Party Conversations: A Diarization-Free Framework for ASR
  </title>

  <style>
    body { 
      font-family: system-ui, -apple-system, Arial; 
      max-width: 1000px; 
      margin: 40px auto; 
      padding: 0 20px; 
      line-height: 1.6;
    }

    h1 { 
      font-size: 28px; 
      font-weight: 600; 
      margin-bottom: 20px; 
    }

    h2 { margin-top: 0; }

    .section {
      margin: 50px 0;
    }

    .abstract-box {
      background: #f8f9fb;
      padding: 20px;
      border-radius: 10px;
      border: 1px solid #e3e6ea;
      text-align: justify;
    }

    .figure-box {
      text-align: center;
    }

    .figure-box img {
      max-width: 100%;
      border-radius: 12px;
      border: 1px solid #ddd;
    }

    .caption {
      font-size: 14px;
      color: #666;
      margin-top: 8px;
    }

    .card { 
      border: 1px solid #eee; 
      border-radius: 12px; 
      padding: 20px; 
      margin: 24px 0; 
    }

    .original-box {
      background: #fafafa;
      border: 1px solid #ddd;
    }

    audio { 
      width: 100%; 
      margin-bottom: 14px; 
    }
  </style>
</head>

<body>

  <h1>
    Position-Aware Target Speaker Extraction for Long-Form Multi-Party Conversations: 
    A Diarization-Free Framework for ASR
  </h1>

  <!-- Abstract -->
  <div class="section">
    <h2>Abstract</h2>
    <div class="abstract-box">
      <p>
      In long-form multi-party conversations, highly imbalanced speaker activity and frequent overlap make it difficult to identify “who spoke when and what”. Sliding-window continuous speech separation (CSS) mitigates sparse supervision, but often suffers from cross-window speaker inconsistency and residual crosstalk, which in practice requires diarization for reliable speaker attribution. Motivated by the stability of speaker directions of arrival (DOAs) in meetings, we propose PATSE, a multi-channel position-aware target speaker extraction front-end that uses DOA as an explicit spatial prior to directly extract speech of each target speaker. PATSE combines a DOA-guided spatial encoder with FiLM-based conditioning to produce speaker-attributed streams, from which speaker activity can be obtained via simple post-processing (e.g., VAD) without explicit diarization. Experiments on both replayed and real conversations show consistent ASR gains outperforming CSS and diarization-based pipelines.
      </p>
    </div>
  </div>

  <!-- Model Architecture -->
  <div class="section">
    <h2>Model Architecture</h2>
    <div class="figure-box">
      <img src="model.jpg" alt="Model Architecture Diagram">
      <div class="caption">
        Figure 1. Overview of the proposed PATSE framework.
      </div>
    </div>
  </div>

  <!-- Audio Demo -->
  <div class="section">
    <h2>Audio Demo</h2>

    <div class="card original-box">
      <h3>Original Audio</h3>
      <audio controls preload="metadata">
        <source src="original.wav" type="audio/wav">
      </audio>
    </div>

    <div class="card">
      <h3>Method 1</h3>
      <p><strong>Speaker A</strong></p>
      <audio controls preload="metadata">
        <source src="method1_A.wav" type="audio/wav">
      </audio>
    </div>

  </div>

</body>
</html>
