<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Position-Aware Target Speaker Extraction for Long-Form Multi-Party Conversations</title>

  <style>
    body { 
      font-family: system-ui, -apple-system, Arial; 
      max-width: 1000px; 
      margin: 40px auto; 
      padding: 0 20px; 
      line-height: 1.6;
    }

    h1 { margin-bottom: 6px; }
    h2 { margin-top: 0; }

    .meta { 
      color: #666; 
      font-size: 14px; 
      margin-bottom: 24px; 
    }

    .section {
      margin: 40px 0;
    }

    .card { 
      border: 1px solid #eee; 
      border-radius: 12px; 
      padding: 20px; 
      margin: 24px 0; 
    }

    .original-box {
      background: #fafafa;
      border: 1px solid #ddd;
    }

    audio { 
      width: 100%; 
      margin-bottom: 14px; 
    }

    .abstract-box {
      background: #f8f9fb;
      padding: 20px;
      border-radius: 10px;
      border: 1px solid #e3e6ea;
      text-align: justify;
    }

    .figure-box {
      text-align: center;
    }

    .figure-box img {
      max-width: 100%;
      border-radius: 12px;
      border: 1px solid #ddd;
    }

    .caption {
      font-size: 14px;
      color: #666;
      margin-top: 8px;
    }
  </style>
</head>

<body>

  <h1>Position-Aware Target Speaker Extraction for Long-Form Multi-Party Conversations</h1>
  <p class="meta">
    A Diarization-Free Framework for ASR
  </p>

  <!-- Abstract -->
  <div class="section">
    <h2>Abstract</h2>
    <div class="abstract-box">
      <p>
        We propose a Position-Aware Target Speaker Extraction (PATSE) framework 
        for long-form multi-party conversations. Unlike conventional approaches 
        that rely on explicit speaker diarization, our method directly extracts 
        the target speaker’s speech conditioned on positional cues, enabling 
        robust and efficient ASR in complex conversational scenarios. 
        Experimental results demonstrate significant improvements in 
        transcription accuracy while simplifying the processing pipeline.
      </p>
    </div>
  </div>

  <!-- Model Figure -->
  <div class="section">
    <h2>Model Architecture</h2>
    <div class="figure-box">
      <!-- 替换为你的模型图文件名 -->
      <img src="model_architecture.png" alt="Model Architecture Diagram">
      <div class="caption">
        Figure 1. Overview of the proposed PATSE framework.
      </div>
    </div>
  </div>

  <!-- Audio Demo Section -->
  <div class="section">
    <h2>Audio Demo</h2>
    <p class="meta">Same original demo, five different methods</p>

    <!-- Original Audio -->
    <div class="card original-box">
      <h3>Original Audio</h3>
      <audio controls preload="metadata">
        <source src="original.wav" type="audio/wav">
      </audio>
    </div>

    <!-- Method 1 -->
    <div class="card">
      <h3>Method 1</h3>

      <p><strong>Speaker A</strong></p>
      <audio controls preload="metadata">
        <source src="method1_A.wav" type="audio/wav">
      </audio>

      <p><strong>Speaker B</strong></p>
      <audio controls preload="metadata">
        <source src="method1_B.wav" type="audio/wav">
      </audio>

      <p><strong>Speaker C</strong></p>
      <audio controls preload="metadata">
        <source src="method1_C.wav" type="audio/wav">
      </audio>
    </div>

    <!-- 继续添加 Method 2-5 -->
  </div>

</body>
</html>
